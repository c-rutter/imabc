@article{mandrik_calibrating_2022,
	title = {Calibrating {Natural} {History} of {Cancer} {Models} in the {Presence} of {Data} {Incompatibility}: {Problems} and {Solutions}},
	volume = {40},
	issn = {1179-2027},
	shorttitle = {Calibrating {Natural} {History} of {Cancer} {Models} in the {Presence} of {Data} {Incompatibility}},
	url = {https://doi.org/10.1007/s40273-021-01125-3},
	doi = {10.1007/s40273-021-01125-3},
	abstract = {The calibration of cancer natural history models is often challenged by a lack of representative calibration targets, forcing modellers to rely on potentially incompatible datasets. Using a microsimulation colorectal cancer model as an example, the purposes of this paper are to (1) highlight the reasons for uncertainty in calibration targets, (2) illustrate practical and generalisable approaches for dealing with incompatibility in calibration targets, and (3) discuss the importance of future research in the area of incorporating uncertainty in calibration. The low quality of data and differences in populations, outcome definitions, and healthcare systems may result in incompatibility between the model and the data. Acknowledging reasons for data incompatibility allows assessment of the risk of incompatibility before calibrating the model. Only a few approaches are available to address data incompatibility, for instance addressing biases in calibration targets and their adjustment, relaxing the goodness-of-fit metric, and validation of the calibration targets to the data not used in the calibration. However, these approaches lack explicit comparison and validation, and so more research is needed to describe the nature and causes of indirect uncertainty (i.e. uncertainty that cannot be expressed in absolute quantitative forms) and identify methods for managing this uncertainty in healthcare modelling.},
	language = {en},
	number = {4},
	urldate = {2022-10-05},
	journal = {PharmacoEconomics},
	author = {Mandrik, Olena and Thomas, Chloe and Whyte, Sophie and Chilcott, James},
	month = apr,
	year = {2022},
	pages = {359--366},
	file = {Mandrik et al. - 2022 - Calibrating Natural History of Cancer Models in th.pdf:/Users/cmaerzlu/Zotero/storage/E8IQKGXT/Mandrik et al. - 2022 - Calibrating Natural History of Cancer Models in th.pdf:application/pdf},
}

@article{baker_analyzing_2022,
	title = {Analyzing {Stochastic} {Computer} {Models}: {A} {Review} with {Opportunities}},
	volume = {37},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Analyzing {Stochastic} {Computer} {Models}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-37/issue-1/Analyzing-Stochastic-Computer-Models-A-Review-with-Opportunities/10.1214/21-STS822.full},
	doi = {10.1214/21-STS822},
	abstract = {In modern science, computer models are often used to understand complex phenomena and a thriving statistical community has grown around analyzing them. This review aims to bring a spotlight to the growing prevalence of stochastic computer models—providing a catalogue of statistical methods for practitioners, an introductory view for statisticians (whether familiar with deterministic computer models or not), and an emphasis on open questions of relevance to practitioners and statisticians. Gaussian process surrogate models take center stage in this review, and these, along with several extensions needed for stochastic settings, are explained. The basic issues of designing a stochastic computer experiment and calibrating a stochastic computer model are prominent in the discussion. Instructive examples, with data and code, are used to describe the implementation of, and results from, various methods.},
	number = {1},
	urldate = {2022-10-05},
	journal = {Statistical Science},
	author = {Baker, Evan and Barbillon, Pierre and Fadikar, Arindam and Gramacy, Robert B. and Herbei, Radu and Higdon, David and Huang, Jiangeng and Johnson, Leah R. and Ma, Pulong and Mondal, Anirban and Pires, Bianica and Sacks, Jerome and Sokolov, Vadim},
	month = feb,
	year = {2022},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {agent based model, Calibration, computer experiment, computer model, Emulator, Gaussian process, surrogates, uncertainty quantification},
	pages = {64--89},
	file = {Baker et al. - 2022 - Analyzing Stochastic Computer Models A Review wit.pdf:/Users/cmaerzlu/Zotero/storage/NZDQC576/Baker et al. - 2022 - Analyzing Stochastic Computer Models A Review wit.pdf:application/pdf;Snapshot:/Users/cmaerzlu/Zotero/storage/I83WF2BI/21-STS822.html:text/html},
}

@article{cook_validation_2006,
	title = {Validation of {Software} for {Bayesian} {Models} {Using} {Posterior} {Quantiles}},
	volume = {15},
	issn = {1061-8600},
	url = {https://www.jstor.org/stable/27594203},
	abstract = {This article presents a simulation-based method designed to establish the computational correctness of software developed to fit a specific Bayesian model, capitalizing on properties of Bayesian posterior distributions. We illustrate the validation technique with two examples. The validation method is shown to find errors in software when they exist and, moreover, the validation output can be informative about the nature and location of such errors. We also compare our method with that of an earlier approach.},
	number = {3},
	urldate = {2022-10-28},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Cook, Samantha R. and Gelman, Andrew and Rubin, Donald B.},
	year = {2006},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {675--692},
	file = {Cook et al. - 2006 - Validation of Software for Bayesian Models Using P.pdf:/Users/cmaerzlu/Zotero/storage/RCZDJSGD/Cook et al. - 2006 - Validation of Software for Bayesian Models Using P.pdf:application/pdf;Correction to Cook Gelman and Rubin 2006.pdf:/Users/cmaerzlu/Zotero/storage/CFZKZB5U/Correction to Cook Gelman and Rubin 2006.pdf:application/pdf},
}

@book{gabry_shinystan_2022,
	title = {shinystan: {Interactive} {Visual} and {Numerical} {Diagnostics} and {Posterior} {Analysis} for {Bayesian} {Models}},
	url = {https://CRAN.R-project.org/package=shinystan},
	author = {Gabry, Jonah and Veen, Duco},
	year = {2022},
	annote = {R package version 2.6.0},
}

@misc{stan_development_team_rstan_2022,
	title = {{RStan}: the {R} interface to {Stan}},
	url = {https://mc-stan.org/},
	author = {{Stan Development Team}},
	year = {2022},
	annote = {R package version 2.21.7},
}

@article{gabry_visualization_nodate,
	title = {Visualization in {Bayesian} workflow},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workﬂow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers.},
	language = {en},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	pages = {14},
	file = {Gabry et al. - Visualization in Bayesian workflow.pdf:/Users/cmaerzlu/Zotero/storage/4CUY29F4/Gabry et al. - Visualization in Bayesian workflow.pdf:application/pdf},
}

@article{ozik_population_2021,
	title = {A population data-driven workflow for {COVID}-19 modeling and learning},
	volume = {35},
	issn = {17412846},
	doi = {10.1177/10943420211035164},
	abstract = {CityCOVID is a detailed agent-based model that represents the behaviors and social interactions of 2.7 million residents of Chicago as they move between and colocate in 1.2 million distinct places, including households, schools, workplaces, and hospitals, as determined by individual hourly activity schedules and dynamic behaviors such as isolating because of symptom onset. Disease progression dynamics incorporated within each agent track transitions between possible COVID-19 disease states, based on heterogeneous agent attributes, exposure through colocation, and effects of protective behaviors of individuals on viral transmissibility. Throughout the COVID-19 epidemic, CityCOVID model outputs have been provided to city, county, and state stakeholders in response to evolving decision-making priorities, while incorporating emerging information on SARS-CoV-2 epidemiology. Here we demonstrate our efforts in integrating our high-performance epidemiological simulation model with large-scale machine learning to develop a generalizable, flexible, and performant analytical platform for planning and crisis response.},
	number = {5},
	journal = {International Journal of High Performance Computing Applications},
	author = {Ozik, Jonathan},
	year = {2021},
	keywords = {Agent-based modeling, high-performance computing, machine learning, model exploration, workflows},
	pages = {483--499},
	file = {Attachment:/Users/cmaerzlu/Zotero/storage/2AZREE4Y/Ozik - 2021 - A population data-driven workflow for COVID-19 modeling and learning.pdf:application/pdf},
}

@article{caro_modeling_2012,
	title = {Modeling {Good} {Research} {Practices}—{Overview}},
	volume = {32},
	issn = {0272-989X},
	url = {http://journals.sagepub.com/doi/10.1177/0272989X12454577},
	doi = {10.1177/0272989X12454577},
	abstract = {Models—mathematical frameworks that facilitate estimation of the consequences of health care decisions—have become essential tools for health technology assessment. Evolution of the methods since the first ISPOR modeling task force reported in 2003 has led to a new task force, jointly convened with the Society for Medical Decision Making, and this series of seven papers presents the updated recommendations for best practices in conceptualizing models; implementing state–transition approaches, discrete event simulations, or dynamic transmission models; dealing with uncertainty; and validating and reporting models transparently. This overview introduces the work of the task force, provides all the recommendations, and discusses some quandaries that require further elucidation. The audience for these papers includes those who build models, stakeholders who utilize their results, and, indeed, anyone concerned with the use of models to support decision making.},
	number = {5},
	journal = {Medical Decision Making},
	author = {Caro, J. Jaime and Briggs, Andrew H. and Siebert, Uwe and Kuntz, Karen M.},
	month = sep,
	year = {2012},
	pmid = {22990082},
	keywords = {good practice, guidelines, methods, modeling},
	pages = {667--677},
	file = {Attachment:/Users/cmaerzlu/Zotero/storage/NEHGYHPX/Caro et al. - 2012 - Modeling Good Research Practices—Overview.pdf:application/pdf},
}

@article{rutter_evidence-based_2010,
	title = {An {Evidence}-{Based} {Microsimulation} {Model} for {Colorectal} {Cancer}: {Validation} and {Application}},
	volume = {19},
	issn = {10559965},
	url = {http://cebp.aacrjournals.org/lookup/doi/10.1158/1055-9965.EPI-09-0954},
	doi = {10.1158/1055-9965.EPI-09-0954},
	abstract = {Background: The Colorectal Cancer Simulated Population model for Incidence and Natural history (CRCSPIN) is a new microsimulation model for the natural history of colorectal cancer that can be used for comparative effectiveness studies of colorectal cancer screening modalities. Methods: CRC-SPIN simulates individual event histories associated with colorectal cancer, based on the adenoma-carcinoma sequence: adenoma initiation and growth, development of preclinical invasive colorectal cancer, development of clinically detectable colorectal cancer, death from colorectal cancer, and death from other causes.We present the CRC-SPIN structure and parameters, data used for model calibration, and model validation. We also provide basic model outputs to further describe CRC-SPIN, including annual transition probabilities between various disease states and dwell times. We conclude with a simple application that predicts the impact of a one-time colonoscopy at age 50 on the incidence of colorectal cancer assuming three different operating characteristics for colonoscopy. Results: CRC-SPIN provides good prediction of both the calibration and the validation data. Using CRCSPIN, we predict that a one-time colonoscopy greatly reduces colorectal cancer incidence over the subsequent 35 years. Conclusions: CRC-SPIN is a valuable new tool for combining expert opinion with observational and experimental results to predict the comparative effectiveness of alternative colorectal cancer screening modalities. Impact: Microsimulation models such as CRC-SPIN can serve as a bridge between screening and treatment studies and health policy decisions by predicting the comparative effectiveness of different interventions. As such, it is critical to publish model descriptions that provide insight into underlying assumptions along with validation studies showing model performance. ©2010 AACR.},
	number = {8},
	journal = {Cancer Epidemiology Biomarkers and Prevention},
	author = {Rutter, Carolyn M. and Savarino, James E.},
	month = aug,
	year = {2010},
	pmid = {20647403},
	pages = {1992--2002},
	file = {Attachment:/Users/cmaerzlu/Zotero/storage/LW4JMFXW/Rutter, Savarino - 2010 - An evidence-based microsimulation model for colorectal cancer Validation and application(2).pdf:application/pdf;Attachment:/Users/cmaerzlu/Zotero/storage/PWA6UY2F/Rutter, Savarino - 2010 - An Evidence-Based Microsimulation Model for Colorectal Cancer Validation and Application.pdf:application/pdf},
}

@article{knudsen_colorectal_2021,
	title = {Colorectal {Cancer} {Screening}: {An} {Updated} {Modeling} {Study} for the {US} {Preventive} {Services} {Task} {Force}},
	volume = {325},
	issn = {15383598},
	doi = {10.1001/jama.2021.5746},
	abstract = {Importance: The US Preventive Services Task Force (USPSTF) is updating its 2016 colorectal cancer screening recommendations. Objective: To provide updated model-based estimates of the benefits, burden, and harms of colorectal cancer screening strategies and to identify strategies that may provide an efficient balance of life-years gained (LYG) from screening and colonoscopy burden to inform the USPSTF. Design, Setting, and Participants: Comparative modeling study using 3 microsimulation models of colorectal cancer screening in a hypothetical cohort of 40-year-old US individuals at average risk of colorectal cancer. Exposures: Screening from ages 45, 50, or 55 years to ages 70, 75, 80, or 85 years with fecal immunochemical testing (FIT), multitarget stool DNA testing, flexible sigmoidoscopy alone or with FIT, computed tomography colonography, or colonoscopy. All persons with an abnormal noncolonoscopy screening test result were assumed to undergo follow-up colonoscopy. Screening intervals varied by test. Full adherence with all procedures was assumed. Main Outcome and Measures: Estimated LYG relative to no screening (benefit), lifetime number of colonoscopies (burden), number of complications from screening (harms), and balance of incremental burden and benefit (efficiency ratios). Efficient strategies were those estimated to require fewer additional colonoscopies per additional LYG relative to other strategies. Results: Estimated LYG from screening strategies ranged from 171 to 381 per 1000 40-year-olds. Lifetime colonoscopy burden ranged from 624 to 6817 per 1000 individuals, and screening complications ranged from 5 to 22 per 1000 individuals. Among the 49 strategies that were efficient options with all 3 models, 41 specified screening beginning at age 45. No single age to end screening was predominant among the efficient strategies, although the additional LYG from continuing screening after age 75 were generally small. With the exception of a 5-year interval for computed tomography colonography, no screening interval predominated among the efficient strategies for each modality. Among the strategies highlighted in the 2016 USPSTF recommendation, lowering the age to begin screening from 50 to 45 years was estimated to result in 22 to 27 additional LYG, 161 to 784 additional colonoscopies, and 0.1 to 2 additional complications per 1000 persons (ranges are across screening strategies, based on mean estimates across models). Assuming full adherence, screening outcomes and efficient strategies were similar by sex and race and across 3 scenarios for population risk of colorectal cancer. Conclusions and Relevance: This microsimulation modeling analysis suggests that screening for colorectal cancer with stool tests, endoscopic tests, or computed tomography colonography starting at age 45 years provides an efficient balance of colonoscopy burden and life-years gained.},
	number = {19},
	journal = {JAMA - Journal of the American Medical Association},
	author = {Knudsen, Amy B. and Rutter, Carolyn M. and Peterse, Elisabeth F.P. and Lietz, Anna P. and Seguin, Claudia L. and Meester, Reinier G.S. and Perdue, Leslie A. and Lin, Jennifer S. and Siegel, Rebecca L. and Doria-Rose, V. Paul and Feuer, Eric J. and Zauber, Ann G. and Kuntz, Karen M. and Lansdorp-Vogelaar, Iris},
	year = {2021},
	pmid = {34003219},
	pages = {1998--2011},
	file = {Attachment:/Users/cmaerzlu/Zotero/storage/IJPJCGPM/Knudsen et al. - 2021 - Colorectal Cancer Screening An Updated Modeling Study for the US Preventive Services Task Force.pdf:application/pdf},
}

@article{rutter_microsimulation_2019,
	title = {Microsimulation model calibration using incremental mixture approximate bayesian computation},
	volume = {13},
	issn = {19417330},
	doi = {10.1214/19-AOAS1279},
	abstract = {Microsimulation models (MSMs) are used to inform policy by predicting population-level outcomes under different scenarios. MSMs simulate individual-level event histories that mark the disease process (such as the development of cancer) and the effect of policy actions (such as screening) on these events. MSMs often have many unknown parameters; calibration is the process of searching the parameter space to select parameters that result in accurate MSM prediction of a wide range of targets. We develop Incremental Mixture Approximate Bayesian Computation (IMABC) for MSM calibration which results in a simulated sample from the posterior distribution of model parameters given calibration targets. IMABC begins with a rejection-based ABC step, drawing a sample of points from the prior distribution of model parameters and accepting points that result in simulated targets that are near observed targets. Next, the sample is iteratively updated by drawing additional points from a mixture of multivariate normal distributions and accepting points that result in accurate predictions. Posterior estimates are obtained by weighting the final set of accepted points to account for the adaptive sampling scheme. We demonstrate IMABC by calibrating CRC-SPIN 2.0, an updated version of a MSM for colorectal cancer (CRC) that has been used to inform national CRC screening guidelines.},
	number = {4},
	journal = {Annals of Applied Statistics},
	author = {Rutter, Carolyn M. and Ozik, Jonathan and Deyoreo, Maria and Collier, Nicholson},
	year = {2019},
	note = {\_eprint: 1804.02090},
	keywords = {Adaptive ABC, Agent-based models, Colorectal cancer},
	pages = {2189--2212},
	file = {Attachment:/Users/cmaerzlu/Zotero/storage/6XVDH3F8/Rutter et al. - 2019 - Microsimulation model calibration using incremental mixture approximate bayesian computation.pdf:application/pdf},
}

@article{rutter_too_2021,
	title = {Too {Good} to {Be} {True}? {Evaluation} of {Colonoscopy} {Sensitivity} {Assumptions} {Used} in {Policy} {Models}},
	issn = {1055-9965},
	url = {http://cebp.aacrjournals.org/lookup/doi/10.1158/1055-9965.EPI-21-1001},
	doi = {10.1158/1055-9965.EPI-21-1001},
	journal = {Cancer Epidemiology Biomarkers \& Prevention},
	author = {Rutter, Carolyn M. and Nascimento de Lima, Pedro and Lee, Jeffrey K. and Ozik, Jonathan},
	month = dec,
	year = {2021},
}

@article{tatara_application_2021,
	title = {Application of {Distributed} {Agent}-based {Modeling} to {Investigate} {Opioid} {Use} {Outcomes} in {Justice} {Involved} {Populations}},
	volume = {2021},
	issn = {2164-7062},
	url = {https://europepmc.org/articles/PMC9297575},
	doi = {10.1109/ipdpsw52791.2021.00157},
	abstract = {Criminal justice involved (CJI) individuals with a history of opioid use disorder (OUD) are at high risk of overdose and death in the weeks following release from jail. We developed the Justice-Community Circulation Model (JCCM) to investigate OUD/CJI dynamics post-release and the effects of interventions on overdose deaths. The JCCM uses a synthetic agent-based model population of approximately 150,000 unique individuals that is generated using demographic information collected from multiple Chicago-area studies and data sets. We use a high-performance computing (HPC) workflow to implement a sequential approximate Bayesian computation algorithm for calibrating the JCCM. The calibration results in the simulated joint posterior distribution of the JCCM input parameters. The calibrated model is used to investigate the effects of a naloxone intervention for a mass jail release. The simulation results show the degree to which a targeted intervention focusing on recently released jail inmates can help reduce the risk of death from opioid overdose.},
	journal = {IEEE International Symposium on Parallel \&amp; Distributed Processing, Workshops and Phd Forum : [proceedings]. IEEE International Symposium on Parallel \&amp; Distributed Processing, Workshops and Phd Forum},
	author = {Tatara, Eric and Schneider, John and Quasebarth, Madeline and Collier, Nicholson and Pollack, Harold and Boodram, Basmattee and Friedman, Sam and Salisbury-Afshar, Elizabeth and Mackesy-Amiti, Mary Ellen and Ozik, Jonathan},
	month = jun,
	year = {2021},
	pages = {989--997},
}

@article{nascimento_de_lima_reopening_2021,
	title = {Reopening {California}: {Seeking} robust, non-dominated {COVID}-19 exit strategies},
	volume = {16},
	issn = {1932-6203},
	shorttitle = {Reopening {California}},
	url = {https://dx.plos.org/10.1371/journal.pone.0259166},
	doi = {10.1371/journal.pone.0259166},
	abstract = {The COVID-19 pandemic required significant public health interventions from local governments. Although nonpharmaceutical interventions often were implemented as decision rules, few studies evaluated the robustness of those reopening plans under a wide range of uncertainties. This paper uses the Robust Decision Making approach to stress-test 78 alternative reopening strategies, using California as an example. This study uniquely considers a wide range of uncertainties and demonstrates that seemingly sensible reopening plans can lead to both unnecessary COVID-19 deaths and days of interventions. We find that plans using fixed COVID-19 case thresholds might be less effective than strategies with time-varying reopening thresholds. While we use California as an example, our results are particularly relevant for jurisdictions where vaccination roll-out has been slower. The approach used in this paper could also prove useful for other public health policy problems in which policymakers need to make robust decisions in the face of deep uncertainty.},
	language = {en},
	number = {10},
	urldate = {2022-11-04},
	journal = {PLOS ONE},
	author = {Nascimento de Lima, Pedro and Lempert, Robert and Vardavas, Raffaele and Baker, Lawrence and Ringel, Jeanne and Rutter, Carolyn M. and Ozik, Jonathan and Collier, Nicholson},
	editor = {Pamucar, Dragan},
	month = oct,
	year = {2021},
	pages = {e0259166},
}

@article{raftery_estimating_2010,
	title = {Estimating and projecting trends in {HIV}/{AIDS} generalized epidemics using incremental mixture importance sampling},
	volume = {66},
	url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&dopt=Citation&list_uids=20222935},
	doi = {BIOM1399 [pii] 10.1111/j.1541-0420.2010.01399.x},
	abstract = {Summary. The Joint United Nations Programme on HIV/AIDS (UNAIDS) has decided to use Bayesian melding as the basis for its probabilistic projections of HIV prevalence in countries with generalized epidemics. This combines a mechanistic epidemiological model, prevalence data, and expert opinion. Initially, the posterior distribution was approximated by sampling-importance-resampling, which is simple to implement, easy to interpret, transparent to users, and gave acceptable results for most countries. For some countries, however, this is not computationally efficient because the posterior distribution tends to be concentrated around nonlinear ridges and can also be multimodal. We propose instead incremental mixture importance sampling (IMIS), which iteratively builds up a better importance sampling function. This retains the simplicity and transparency of sampling importance resampling, but is much more efficient computationally. It also leads to a simple estimator of the integrated likelihood that is the basis for Bayesian model comparison and model averaging. In simulation experiments and on real data, it outperformed both sampling importance resampling and three publicly available generic Markov chain Monte Carlo algorithms for this kind of problem.},
	language = {Eng},
	number = {4},
	journal = {Biometrics},
	author = {Raftery, A E and Bao, L},
	year = {2010},
	pmid = {20222935},
	note = {ISBN: 1541-0420 (Electronic) 0006-341X (Linking)},
	pages = {1162--1173},
	file = {Raftery and Bao - 2010 - Estimating and projecting trends in HIVAIDS gener.pdf:/Users/cmaerzlu/Zotero/storage/2UVU8VFZ/Raftery and Bao - 2010 - Estimating and projecting trends in HIVAIDS gener.pdf:application/pdf},
}

@article{deyoreo_sequentially_2022,
	title = {Sequentially calibrating a {Bayesian} microsimulation model to incorporate new information and assumptions},
	volume = {22},
	issn = {1472-6947},
	url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-021-01726-0},
	doi = {10.1186/s12911-021-01726-0},
	number = {1},
	journal = {BMC Medical Informatics and Decision Making},
	author = {DeYoreo, Maria and Rutter, Carolyn M and Ozik, Jonathan and Collier, Nicholson},
	month = dec,
	year = {2022},
	pmcid = {PMC8756687},
	pmid = {35022005},
	pages = {12},
	file = {DeYoreo et al. - 2022 - Sequentially calibrating a Bayesian microsimulatio.pdf:/Users/cmaerzlu/Zotero/storage/6C8AVEAM/DeYoreo et al. - 2022 - Sequentially calibrating a Bayesian microsimulatio.pdf:application/pdf},
}

@misc{mao_detecting_2020,
	title = {Detecting conflicting summary statistics in likelihood-free inference},
	url = {http://arxiv.org/abs/2010.07465},
	abstract = {Bayesian likelihood-free methods implement Bayesian inference using simulation of data from the model to substitute for intractable likelihood evaluations. Most likelihood-free inference methods replace the full data set with a summary statistic before performing Bayesian inference, and the choice of this statistic is often difficult. The summary statistic should be low-dimensional for computational reasons, while retaining as much information as possible about the parameter. Using a recent idea from the interpretable machine learning literature, we develop some regression-based diagnostic methods which are useful for detecting when different parts of a summary statistic vector contain conflicting information about the model parameters. Conflicts of this kind complicate summary statistic choice, and detecting them can be insightful about model deficiencies and guide model improvement. The diagnostic methods developed are based on regression approaches to likelihood-free inference, in which the regression model estimates the posterior density using summary statistics as features. Deletion and imputation of part of the summary statistic vector within the regression model can remove conflicts and approximate posterior distributions for summary statistic subsets. A larger than expected change in the estimated posterior density following deletion and imputation can indicate a conflict in which inferences of interest are affected. The usefulness of the new methods is demonstrated in a number of real examples.},
	urldate = {2022-10-08},
	publisher = {arXiv},
	author = {Mao, Yinan and Wang, Xueou and Nott, David J. and Evans, Michael},
	month = oct,
	year = {2020},
	note = {arXiv:2010.07465 [stat]},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/cmaerzlu/Zotero/storage/RPZRYS4V/Mao et al. - 2020 - Detecting conflicting summary statistics in likeli.pdf:application/pdf;arXiv.org Snapshot:/Users/cmaerzlu/Zotero/storage/JFRRURIE/2010.html:text/html},
}

@article{nowak_using_2021,
	title = {Using an {Agent}-based {Model} to {Examine} {Deimplementation} of {Breast} {Cancer} {Screening}},
	volume = {59},
	issn = {0025-7079},
	url = {https://journals.lww.com/10.1097/MLR.0000000000001442},
	doi = {10.1097/MLR.0000000000001442},
	language = {en},
	number = {1},
	urldate = {2022-10-18},
	journal = {Medical Care},
	author = {Nowak, Sarah A. and Parker, Andrew M. and Radhakrishnan, Archana and Schoenborn, Nancy and Pollack, Craig E.},
	month = jan,
	year = {2021},
	pages = {e1--e8},
	file = {mlr_2020_10_03_nowak_mdc-d-20-00228_sdc1.pdf:/Users/cmaerzlu/Zotero/storage/KNE3Z2EQ/mlr_2020_10_03_nowak_mdc-d-20-00228_sdc1.pdf:application/pdf;Nowak et al. - 2021 - Using an Agent-based Model to Examine Deimplementa.pdf:/Users/cmaerzlu/Zotero/storage/8IKFT2GY/Nowak et al. - 2021 - Using an Agent-based Model to Examine Deimplementa.pdf:application/pdf},
}

@article{ozik_population_2021-1,
	title = {A population data-driven workflow for {COVID}-19 modeling and learning},
	volume = {35},
	issn = {1094-3420, 1741-2846},
	url = {http://journals.sagepub.com/doi/10.1177/10943420211035164},
	doi = {10.1177/10943420211035164},
	abstract = {CityCOVID is a detailed agent-based model that represents the behaviors and social interactions of 2.7 million residents of Chicago as they move between and colocate in 1.2 million distinct places, including households, schools, workplaces, and hospitals, as determined by individual hourly activity schedules and dynamic behaviors such as isolating because of symptom onset. Disease progression dynamics incorporated within each agent track transitions between possible COVID-19 disease states, based on heterogeneous agent attributes, exposure through colocation, and effects of protective behaviors of individuals on viral transmissibility. Throughout the COVID-19 epidemic, CityCOVID model outputs have been provided to city, county, and state stakeholders in response to evolving decision-making priorities, while incorporating emerging information on SARS-CoV-2 epidemiology. Here we demonstrate our efforts in integrating our high-performance epidemiological simulation model with largescale machine learning to develop a generalizable, ﬂexible, and performant analytical platform for planning and crisis response.},
	language = {en},
	number = {5},
	urldate = {2022-10-19},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Ozik, Jonathan and Wozniak, Justin M and Collier, Nicholson and Macal, Charles M and Binois, Mickaël},
	month = sep,
	year = {2021},
	pages = {483--499},
	file = {Ozik et al. - 2021 - A population data-driven workflow for COVID-19 mod.pdf:/Users/cmaerzlu/Zotero/storage/WZH8ZW3S/Ozik et al. - 2021 - A population data-driven workflow for COVID-19 mod.pdf:application/pdf},
}

@article{ellis_active_2020,
	title = {Active learning for efficiently training emulators of computationally expensive mathematical models},
	volume = {39},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8679},
	doi = {10.1002/sim.8679},
	abstract = {An emulator is a fast-to-evaluate statistical approximation of a detailed mathematical model (simulator). When used in lieu of simulators, emulators can expedite tasks that require many repeated evaluations, such as sensitivity analyses, policy optimization, model calibration, and value-of-information analyses. Emulators are developed using the output of simulators at specific input values (design points). Developing an emulator that closely approximates the simulator can require many design points, which becomes computationally expensive. We describe a self-terminating active learning algorithm to efficiently develop emulators tailored to a specific emulation task, and compare it with algorithms that optimize geometric criteria (random latin hypercube sampling and maximum projection designs) and other active learning algorithms (treed Gaussian Processes that optimize typical active learning criteria). We compared the algorithms' root mean square error (RMSE) and maximum absolute deviation from the simulator (MAX) for seven benchmark functions and in a prostate cancer screening model. In the empirical analyses, in simulators with greatly varying smoothness over the input domain, active learning algorithms resulted in emulators with smaller RMSE and MAX for the same number of design points. In all other cases, all algorithms performed comparably. The proposed algorithm attained satisfactory performance in all analyses, had smaller variability than the treed Gaussian Processes, and, on average, had similar or better performance as the treed Gaussian Processes in six out of seven benchmark functions and in the prostate cancer model.},
	language = {en},
	number = {25},
	urldate = {2022-11-08},
	journal = {Statistics in Medicine},
	author = {Ellis, Alexandra G. and Iskandar, Rowan and Schmid, Christopher H. and Wong, John B. and Trikalinos, Thomas A.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8679},
	keywords = {adaptive design, kernel methods, kriging, meta-model, sequential design, surrogate model},
	pages = {3521--3548},
	file = {Ellis et al. - 2020 - Active learning for efficiently training emulators.pdf:/Users/cmaerzlu/Zotero/storage/EX86E5VL/Ellis et al. - 2020 - Active learning for efficiently training emulators.pdf:application/pdf},
}

@article{dalmasso_validation_2020,
	title = {Validation of {Approximate} {Likelihood} and {Emulator} {Models} for {Computationally} {Intensive} {Simulations}},
	volume = {108},
	abstract = {Complex phenomena in engineering and the sciences are often modeled with computationally intensive feed-forward simulations for which a tractable analytic likelihood does not exist. In these cases, it is sometimes necessary to estimate an approximate likelihood or ﬁt a fast emulator model for eﬃcient statistical inference; such surrogate models include Gaussian synthetic likelihoods and more recently neural density estimators such as autoregressive models and normalizing ﬂows. To date, however, there is no consistent way of quantifying the quality of such a ﬁt. Here we propose a statistical framework that can distinguish any arbitrary misspeciﬁed model from the target likelihood, and that in addition can identify with statistical conﬁdence the regions of parameter as well as feature space where the ﬁt is inadequate. At the heart of our approach is a two-sample test that quantiﬁes the quality of the ﬁt at ﬁxed parameter values, and a global test that assesses goodness-of-ﬁt across simulation parameters. While our general framework can incorporate any test statistic or distance metric, we speciﬁcally argue for a new two-sample test that can leverage any regression method to attain high power and provide diagnostics in complex data settings. Software for our approach is available on GitHub in Python and R.},
	language = {en},
	journal = {Proceedings of the 23rdInternational Conference on Artificial Intelligence and Statistics (AISTATS)},
	author = {Dalmasso, Niccolo and Lee, Ann B and Izbicki, Rafael and Pospisil, Taylor and Kim, Ilmun and Lin, Chieh-An},
	year = {2020},
	pages = {12},
	file = {Dalmasso et al. - Validation of Approximate Likelihood and Emulator .pdf:/Users/cmaerzlu/Zotero/storage/82HFJNAE/Dalmasso et al. - Validation of Approximate Likelihood and Emulator .pdf:application/pdf},
}
